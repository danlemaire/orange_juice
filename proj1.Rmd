---
title: "Project 1"
output: html_notebook
---

```{r Load Data, message=FALSE, warning=FALSE, include=FALSE}

library(magrittr)
library(tidyverse)
library(caret)
library(broom)
library(ROCR)
library(h2o)

#Set seed now before training split, and again before training each model to compare with others
set.seed(444)

#Load data
df <- read_csv("oj.csv") %>% 
  map_at(c("Purchase", "Store7", "STORE", "StoreID"), as.factor) %>% 
  as_tibble %>% 
  mutate(WeekofPurchase = WeekofPurchase - min(WeekofPurchase)) %>% 
  select(-STORE, -Store7)

#Build train and test set 
df <- df[!duplicated(df),] #Found some duplicates in rows
train <- df %>% sample_frac(.8)
test  <- df %>% setdiff(train)
```



```{r Build Logistic Model in Caret, message=FALSE, warning=FALSE, include=FALSE}

## Create trainControl
trainControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 3,
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE,
                           returnResamp = "all"
                           )

# Build Logistic model and look at results
set.seed(444)
logistic_model <- train(Purchase ~ .,  
                data = train, 
                method = "glm", 
                family = "binomial", 
                trControl = trainControl,
                metric = "ROC",
                preProcess = c("center", "scale")
                )

test %<>% mutate(glm_preds = factor(predict(logistic_model, newdata = test)))
glm_cfm <- confusionMatrix(test$glm_preds, test$Purchase)

#Look at variable importances and coefficients for logistic model
varImp(logistic_model) %>% plot(main = "Variable Importances for Logistic Model")

#Consider performance of the model on test set
glm_auc <- (predict(logistic_model, test, type = "prob") %>% 
  select(MM) %>% 
  prediction(test$Purchase) %>% 
  performance("auc"))@y.values[[1]][1]



```


```{r Build Logistic Model with glmnet, message=FALSE, warning=FALSE, include=FALSE}

## Create tuneGrid
tuneGrid = expand.grid(.alpha = seq(.2, .55, length = 15), 
                       .lambda = seq(0.025, .125, length = 10)
                       )

# Build Logistic model and look at results
set.seed(444)
glmnet_model <- train(Purchase ~ .,  
                data = train, 
                method = "glmnet", 
                family = "binomial", 
                trControl = trainControl,
                tuneGrid = tuneGrid,
                metric = "ROC",
                preProcess = c("center", "scale")
                )
glmnet_model

test %<>% mutate(glmnet_preds = factor(predict(glmnet_model, newdata = test)))
confusionMatrix(data = test$glmnet_preds, test$Purchase)

varImp(glmnet_model) %>% plot(main = "Variable Importances for GLMNET Model")

#Best alpha
ggplot() +
  geom_line(data = glmnet_model$results %>% group_by(alpha) %>% summarise(ROC = median(ROC)),
              aes(alpha, ROC))
#Best lambda
ggplot() +
  geom_line(data = glmnet_model$results %>% group_by(lambda) %>% summarise(ROC = median(ROC)),
              aes(lambda, ROC))

#Consider performance of the model
glmnet_auc <- (predict(glmnet_model, test, type = "prob") %>% 
  select(MM) %>% 
  prediction(test$Purchase) %>% 
  performance("auc"))@y.values[[1]][1]

#Test parameters for statistically significant difference in auc performance
glmnet_model$resample %>% 
  lm(ROC ~ factor(alpha) + factor(lambda), .) %>% 
  summary %>% 
  tidy %>% 
  arrange(p.value)

#Visualize distributions of performance for each model (10 folds * 3 repeats = 30 runs of each model)
glmnet_model$resample %>% 
  select(alpha, lambda) %>% 
  distinct %>%
  mutate(model_number = 1:nrow(.)) %>% 
  right_join(glmnet_model$resample, by = c("alpha", "lambda")) %>% 
  ggplot(aes(ROC, group = model_number, color = model_number)) + 
  geom_density() +
  scale_alpha_continuous(range = c(0.1, 1)) +
  geom_vline(xintercept = (glmnet_model$resample %>% group_by(alpha, lambda) %>% summarise(median(ROC)) %>% max), color = "red") + 
  geom_vline(xintercept = (glmnet_model$resample %>% group_by(alpha, lambda) %>% summarise(roc = median(ROC)) %>% ungroup %>% summarise(min(roc)) %>% min), color = "red") +
  labs(title = "Difference between models is not statistically significant",
       subtitle = "Of the 150 models, the best and worst performers are still very close")

```


```{r Build Support Vector Machine, message=FALSE, warning=FALSE, include=FALSE}

## Update trainControl
trainControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 3,
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE,
                           returnResamp = "all"
                           )

## Update tuneGrid
tuneGrid = expand.grid(sigma = seq(.01, 1, length = 10), 
                       C = seq(0.5, 10, length = 10)
                       )


###### Build SVM model
set.seed(444)
svm_model <- train(Purchase ~ ., 
                 data = train, 
                 method = 'svmRadial',  
                 trControl = trainControl,
                 preProc = c("center","scale"),
                 metric = "ROC",
                 verbose = FALSE,
                 probability = TRUE,
                 tuneGrid = tuneGrid
                 )

## Consider performance of model

test %<>% mutate(svm_preds = factor(predict(svm_model, newdata = test, probability = TRUE)))
confusionMatrix(data = test$svm_preds, test$Purchase)

svm_auc <- (predict(svm_model, test, type = "prob") %>% 
  select(MM) %>% 
  prediction(test$Purchase) %>% 
  performance("auc"))@y.values[[1]][1]

```


```{r Build Random Forest, message=FALSE, warning=FALSE, include=FALSE}

## Update trainControl
trainControl <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 3,
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE,
                           returnResamp = "all"
                           )

mtry <- sqrt(ncol(train))
tuneGrid <- expand.grid(.mtry = mtry)

###### Build RF model
set.seed(444)
rf_model <- train(Purchase ~ ., 
                 data = train, 
                 method = 'rf',  
                 trControl = trainControl,
                 metric = "ROC",
                 verbose = FALSE,
                 probability = TRUE,
                 tuneGrid = tuneGrid
                 )

## Consider performance of model
test$rf_preds <- predict(rf_model, newdata = test, probability = TRUE)
confusionMatrix(data = test$rf_preds, test$Purchase)
rf_auc <- (predict(rf_model, test, type = "prob") %>% 
  select(MM) %>% 
  prediction(test$Purchase) %>% 
  performance("auc"))@y.values[[1]][1]


## Used to visualize and select best mtry
# rf_model$results %>% 
#   ggplot(aes(mtry, ROC)) + geom_point()
# 
# rf_model$resample %>% 
#   group_by(mtry) %>% 
#   summarise(roc = median(ROC)) %>% 
#   ggplot(aes(mtry, roc)) + geom_line()

```


```{r Look at errors of each model, message=FALSE, warning=FALSE, include=FALSE}

results <- data_frame(model = c("Basic Logistic",
                                "Elasticnet Logistic",
                                "Support Vector Machine",
                                "Random Forest"),
                      auc = c(glm_auc,
                              glmnet_auc,
                              svm_auc,
                              rf_auc))

```


```{r Build GLM model on an h2o cluster, message=FALSE, warning=FALSE, include=FALSE}

h2o.init(nthreads = -1)


h2o_train <- as.h2o(train)
h2o_test <- as.h2o(test)

set.seed(444)
h2o_glm <- h2o.glm(y = "Purchase", 
                  x = setdiff(colnames(h2o_train), "Purchase"), 
                  training_frame = h2o_train,
                  family = "binomial",
                  nfolds = 3,
                  solver = "IRLSM",
                  lambda = 0,
                  remove_collinear_columns = T,
                  standardize = T,
                  balance_classes = T,
                  fold_assignment = "Stratified",
                  compute_p_values = T
                  )


summary(h2o_glm)

test$h2o_glm_preds <- (h2o_glm %>% h2o.predict(h2o_test))$predict %>% as.vector %>% factor

h2o.varimp(h2o_glm)
h2o.varimp_plot(h2o_glm)

h2o_glm_perf <- h2o.performance(h2o_glm, h2o_test)
h2o_glm_auc <- h2o_glm_perf@metrics$AUC
```


```{r Build GBM model on an h2o cluster, message=FALSE, warning=FALSE, include=FALSE}
set.seed(444)
h2o_gbm <- h2o.gbm(y = "Purchase", 
                  x = setdiff(colnames(h2o_train), "Purchase"), 
                  training_frame = h2o_train,
                  stopping_metric = "AUC",
                  balance_classes = T
                  )
h2o_gbm_perf <- h2o.performance(h2o_gbm, h2o_test)
h2o_gbm_auc <- h2o_gbm_perf@metrics$AUC
```


```{r Build random forest model on an h2o cluster, message=FALSE, warning=FALSE, include=FALSE}
set.seed(444)
h2o_rf <- h2o.randomForest(y = "Purchase", 
                  x = setdiff(colnames(h2o_train), "Purchase"), 
                  training_frame = h2o_train,
                  stopping_metric = "AUC",
                  balance_classes = T,
                  nfolds = 3
                  )
h2o_rf_perf <- h2o.performance(h2o_rf, h2o_test)
h2o_rf_auc <- h2o_rf_perf@metrics$AUC
```


```{r Build deep neural net on an h2o cluster, message=FALSE, warning=FALSE, include=FALSE}
set.seed(444)
h2o_dl <- h2o.deeplearning(y = "Purchase", 
                  x = setdiff(colnames(h2o_train), "Purchase"), 
                  training_frame = h2o_train,
                  epochs = 1000,
                  stopping_metric = "AUC",
                  balance_classes = T
                  )
h2o_dl_perf <- h2o.performance(h2o_dl, h2o_test)
h2o_dl_auc <- h2o_dl_perf@metrics$AUC
```


```{r Add h2o results to previous results dataframe, message=FALSE, warning=FALSE, include=FALSE}
results %<>% bind_rows(data_frame(model = c("h2o GLM", 
                                            "h2o GBM", 
                                            "h2o Random Forest", 
                                            "h2o Neural Net"),
                                  auc = c(h2o_glm_auc, 
                                          h2o_gbm_auc, 
                                          h2o_rf_auc, 
                                          h2o_dl_auc)
                                  )
                       )
                      
```


# Overview

The goal of this analysis is to understand how to make the Orange Juice category of store sales perform better.  The store sells two brands of orange juice, Minute Maid (MM) and Citrus Hill (CH).  Since MM has higher margins than CH, this analysis will make recommendations regarding which factors influence a consumer's decision to purchase MM orange juice.  This allows our compaqny to leverage those factors as opportunities to influence MM sales.  It will additionally provide a predictive model for more precise forecasting.  This forecasting will be of benefit now, but will be of tremendous benefit later when the company adjusts its marketing to increase MM sales (since an updated forecast will be required).


# Problem Definition

Brand manager
1. What predictor variables influence the sales of MM?
```{r, echo=FALSE, message=FALSE, warning=FALSE}

h2o_glm@model$coefficients_table %>% 
  as_tibble %>% 
  mutate(sign = factor(ifelse(standardized_coefficients >= 0, 1, 0)),
         significant = (ifelse(p_value <= .05, 1, 0) %>% factor)) %>%
  filter(!is.na(p_value)) %>% 
  ggplot(aes(x = reorder(names, abs(standardized_coefficients)), 
             y = abs(standardized_coefficients), 
             fill = sign,
             alpha = significant)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = "Discounts, relative prices, and brand loyalty matter",
         y = "Influence of each factor on a customer's decision to purchase Minute Maid",
         x = element_blank()) +
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank(),
          legend.position = c(.85, 0.15),
          legend.title = element_blank(),
          panel.background = element_blank()
          ) +
    scale_fill_discrete(breaks = c(1, 0),
                        labels = c("More Likely to buy MM", "Less likely to buy MM")
                        ) +
    scale_alpha_discrete(guide = "none", range = c(.15, 1))
  
```

2. Are all the variables in the dataset effective or are some more effective than
others?

```{r}
h2o_glm@model$coefficients_table %>% 
  as_tibble %>% 
  transmute(names = names,
            p_value = p_value,
            significant = (ifelse(p_value <= .05, 1, 0) %>% factor),
            removed = ifelse(is.na(p_value), 1, 0)) %>% 
  filter(!is.na(significant)) %>% 
  ggplot(aes(reorder(names, -abs(p_value)), 1 - p_value, fill = significant, label = round(p_value, 2))) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = "Only a few factors in the model are statistically significant",
         y = "Statistical confidence that this result could not have happened by chance",
         x = element_blank()) +
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank(),
          legend.position = c(.75, 0.15),
          legend.title = element_blank(),
          panel.background = element_blank()
          ) +
    geom_text(nudge_y = -.03) +
    scale_fill_discrete(breaks = c(01,0), labels = c("Statistically significant", "Not statistically significant"))
```


3. How confident are you in your recommendations?

```{r}

variance_of_model_df <- data_frame(round = numeric(), performance = numeric())

for (i in 1:30) {
  set.seed(i)
  
  split <- h2o.splitFrame(as.h2o(df), .8)
  h2o_tr <- split[[1]]
  h2o_te <- split[[2]]
  
  h2o_glm <- h2o.glm(y = "Purchase", 
                x = setdiff(colnames(h2o_tr), "Purchase"), 
                training_frame = h2o_tr,
                family = "binomial",
                nfolds = 3,
                solver = "IRLSM",
                lambda = 0,
                remove_collinear_columns = T,
                standardize = T,
                balance_classes = T,
                fold_assignment = "Stratified",
                compute_p_values = T
                )
  variance_of_model_df[i,] <- c(round = i, performance = (h2o.performance(h2o_glm, h2o_te))@metrics$AUC)
  #print(i)
}

variance_of_model_df %>%  
  ggplot(aes(performance)) +
    geom_density()

variance_of_model_df$performance %>% sd




```



4. Based on your analysis what are specific recommendations you have for the
brand manager?






Sales manager
1. Can you provide him a predictive model that can tell him the probability of
customers buying MM?
2. How good is the model in its predictions?
3. How confident are you in your recommendations?



# Methods



# Results



# Recommendations





